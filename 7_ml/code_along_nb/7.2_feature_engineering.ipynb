{"cells":[{"cell_type":"markdown","metadata":{"id":"66pqqERbR_TC"},"source":["## Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"bNwm83KER_TF"},"source":["Feature Engineering is crucial in Machine Learning. It involves selecting, transforming, and creating features from raw data to improve model performance and interpretability. Effective feature engineering can significantly enhance the predictive power and generalization ability of machine learning models."]},{"cell_type":"markdown","metadata":{"id":"geU3scpzR_TF"},"source":["Yesterday, in the KNN Regression aproach, we saw a pretty poor model. Let's apply some feature engineering techniques to see if it improves our model."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>salary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>20</td>\n","      <td>1.81</td>\n","      <td>75</td>\n","      <td>32000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>22</td>\n","      <td>1.83</td>\n","      <td>77</td>\n","      <td>28000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>60</td>\n","      <td>1.98</td>\n","      <td>86</td>\n","      <td>31000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   age  height  weight  salary\n","0   20    1.81      75   32000\n","1   22    1.83      77   28000\n","2   60    1.98      86   31000"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# example to show importance of data scaling\n","\n","my_dict={'age':[20, 22, 60],\n","         'height':[1.81, 1.83, 1.98],\n","         'weight': [75, 77, 86],\n","         'salary': [32000, 28000, 31000]}\n","df= pd.DataFrame.from_dict(my_dict, orient=\"columns\")\n","df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["0       0.000000\n","1    4000.001000\n","2    1000.860145\n","dtype: float64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["def distance_individual_zero(row):\n","    distance = (row[\"age\"]-20)**2 + (row[\"height\"]-1.81)**2 + (row[\"weight\"]-75)**2 + (row[\"salary\"]-32000)**2\n","    return np.sqrt(distance)\n","\n","df.apply(distance_individual_zero, axis=1)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>salary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.05</td>\n","      <td>0.117647</td>\n","      <td>0.181818</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.00</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.75</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    age    height    weight  salary\n","0  0.00  0.000000  0.000000    1.00\n","1  0.05  0.117647  0.181818    0.00\n","2  1.00  1.000000  1.000000    0.75"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","df_scaled = scaler.fit_transform(df)\n","df_scaled = pd.DataFrame(df_scaled, columns = df.columns)\n","df_scaled"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["0    0.000000\n","1    1.024402\n","2    1.750000\n","dtype: float64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["def distance_individual_zero(row):\n","    distance = (row[\"age\"]-0.00)**2 + (row[\"height\"]-0.00)**2 + (row[\"weight\"]-0.00)**2 + (row[\"salary\"]-1)**2\n","    return np.sqrt(distance)\n","\n","df_scaled.apply(distance_individual_zero, axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>salary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.760750</td>\n","      <td>-0.834812</td>\n","      <td>-0.905753</td>\n","      <td>0.980581</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.652071</td>\n","      <td>-0.571187</td>\n","      <td>-0.487713</td>\n","      <td>-1.372813</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.412821</td>\n","      <td>1.405999</td>\n","      <td>1.393466</td>\n","      <td>0.392232</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        age    height    weight    salary\n","0 -0.760750 -0.834812 -0.905753  0.980581\n","1 -0.652071 -0.571187 -0.487713 -1.372813\n","2  1.412821  1.405999  1.393466  0.392232"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","df_scaled = scaler.fit_transform(df)\n","df_scaled = pd.DataFrame(df_scaled, columns = df.columns)\n","df_scaled"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["0    4.335532e-07\n","1    2.407183e+00\n","2    3.921506e+00\n","dtype: float64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["def distance_individual_zero(row):\n","    distance = (row[\"age\"]-(-0.760750))**2 + (row[\"height\"]-(-0.834812))**2 + (row[\"weight\"]-(-0.905753))**2 + (row[\"salary\"]-0.980581)**2\n","    return np.sqrt(distance)\n","\n","df_scaled.apply(distance_individual_zero, axis=1)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["4.335532e-07"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["0.0000004335532"]},{"cell_type":"markdown","metadata":{"id":"cQFKrSS2R_TF"},"source":["#### Loading and preparing the data"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WQl3rX7NR_TF"},"outputs":[],"source":["from sklearn.datasets import  fetch_california_housing\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"XDLWGDmER_TG","outputId":"08f3e5b4-d48a-4f27-d30f-57a173d1561a"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"nEn564aKR_TH"},"source":["#### Checking for anomalies"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0VFrlJX3R_TH","outputId":"e08fc803-b08a-47f3-fdde-a4946ca9943f"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"mrVsJNFhR_TH"},"source":["#### Train Test Split"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5UOl_QkoR_TI"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"at4JizyDR_TI"},"source":["#### Normalization"]},{"cell_type":"markdown","metadata":{"id":"z06GELOUR_TI"},"source":["During normalization or standardization, it's essential to fit the model to the training data exclusively, preventing any exposure to the test data to avoid potential data leakage issues."]},{"cell_type":"markdown","metadata":{"id":"YmBD4IINR_TI"},"source":["Create an instance of the normalizer"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pOd3-i6FR_TI"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"4pUtjfWOR_TI"},"source":["Fit it to our training data"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iD3WJ6ZiR_TI","outputId":"aba0bca2-6bfb-4541-d49b-102b7e48be30"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"x6ihgRt2R_TI"},"source":["Transforming our training and testing data"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"zTXrQY7UR_TI"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"L4xqkOQJR_TI"},"source":["When applying transformations of our dataframe, normalizer will return an array instead of a dataframe object"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"f2D6j6ErR_TI","outputId":"f5382b58-26b9-4a3c-feb0-3d6d2e64b8fd"},"outputs":[],"source":["#your code here"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"iIb89YdJR_TJ","outputId":"ab052281-a562-42ac-f8ac-1558075cd69a"},"outputs":[],"source":["#your code here"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"BOiL_XdwR_TJ","outputId":"f752616c-57ca-42ec-e625-c1a910776320"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"iMZ5ouIsR_TJ"},"source":["##### KNN Regressor - modeling"]},{"cell_type":"markdown","metadata":{"id":"m9MQEw6qR_TJ"},"source":["Let's create an instance of KNN with the same hyperparameter as before, n_neighbors = 10."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"hrB1f4B2R_TJ"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"14hd5Qq3R_TJ"},"source":["Training KNN to our normalized data"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"zuImEavmR_TJ","outputId":"58acd191-3d3d-4af3-de29-cf5bab8852ea"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"R9zsbDM8R_TJ"},"source":["Evaluate model's performance"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"oDQMWcvJR_TJ","outputId":"9d0745ec-f60e-4780-af35-828050bb5e26"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"OZ1nbY9zR_TJ"},"source":["With raw data we obtain a R2 of 0.16, just by normalizing our data, model's perfomance increase a lot to a R2 of 0.70.\n","\n","This happens because KNN is a distance based algorithm, so its suffers a lot with data in completely different scales."]},{"cell_type":"markdown","metadata":{"id":"kdEDyfxvR_TJ"},"source":["## Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"9cA4MW1xR_TJ"},"source":["Even though normalizing our data had a huge impact on KNN performance, we are currently using every single feature of the dataset.\n","\n","Now let's do a selection of features based on correlactions between themselves but also with the target.\n","\n","We want low correlaction between features, but high correlaction between features and our target."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"_p9kostgR_TJ","outputId":"9aa13ecf-1872-4b20-fd1a-f2291c890497"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"S1j-1wcPR_TJ"},"source":["By the correlation matrix we can see that:\n","- \"AveRooms\" is highly correlated with \"AveBedrms\", so we drop the one less correlated with our target\n","- \"AveOccup\" and \"Population\" also have pretty low correlation with our target variable, so lets remove them from our selected features"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"kD2M3pasR_TK"},"outputs":[],"source":["#your code here"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"qelLKY5kR_TN","outputId":"4cf1f5b9-ec8c-428e-8615-c633b7d089f7"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"L_pXy5Z4R_TN"},"source":["By normalizing our data and selecting a subset of available features, we were able to massively improve our model, increasing the R2 score from 0.16 to 0.70\n","\n","Notice that we still haven't fine-tuned our hyperparameter, so we will be able to improve even more our model."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
